{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n# import researchpy as rp\nimport scipy.stats as stats\nimport statsmodels.api as sm\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report,accuracy_score \nimport itertools\n\n# % matplotlib inline\n\ndf = pd.read_csv(\n    '../input/lending-club/accepted_2007_to_2018q4.csv/accepted_2007_to_2018Q4.csv',\n    parse_dates=['issue_d'], infer_datetime_format=True)\ndf = df[(df.issue_d >= '2017-01-01 00:00:00') & (df.issue_d < '2019-01-01 00:00:00')]\ndf = df.reset_index(drop=True)\ndf.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('dti_joint', 'annual_inc_joint', 'il_util', 'mths_since_rcnt_il', 'open_acc_6m', 'open_il_12m',\n           'open_il_24m', 'inq_last_12m', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl',\n           'mths_since_last_record', 'mths_since_last_major_derog', 'mths_since_last_delinq', 'total_bal_il', 'tot_coll_amt',\n           'tot_cur_bal', 'total_rev_hi_lim', 'revol_util', 'collections_12_mths_ex_med', 'open_acc', 'inq_last_6mths',\n           'verification_status_joint', 'acc_now_delinq'):\n    df[col] = df[col].fillna(0)\n    \nmissing_fractions = df.isnull().mean().sort_values(ascending=False)\nmissing_fractions.head(10)\n\nplt.title('Histogram of Feature Incompleteness')\nplt.xlabel('Fraction of data missing')\nplt.ylabel('Feature count')\ndrop_list = sorted(list(missing_fractions[missing_fractions > 0.3].index))\n\ndf.drop(labels=drop_list, axis=1, inplace=True)\n\nkeep_list = ['annual_inc', 'application_type', 'dti', \n             'earliest_cr_line', 'emp_length', 'emp_title', \n             'fico_range_high', 'fico_range_low', 'grade', \n             'home_ownership', 'id', 'initial_list_status', \n             'installment', 'int_rate', 'issue_d', 'loan_amnt', \n             'loan_status', 'mort_acc', 'open_acc', 'pub_rec', \n             'pub_rec_bankruptcies', 'purpose', 'revol_bal', \n             'revol_util', 'sub_grade', 'term', 'title', 'total_acc','verification_status', 'zip_code'\n             ,\"out_prncp\",\"out_prncp_inv\",\"application_type\",\"annual_inc_joint\",\"dti_joint\",'tot_coll_amt'\n,\"open_acc_6m\",\"open_il_6m\",\"open_il_12m\",\"open_il_24m\",\n\"total_bal_il\",\"il_util\",\"open_rv_12m\",\"open_rv_24m\",\"total_rev_hi_lim\",\"inq_fi\",\"inq_last_12m\",\n\"public_record\",\"addr_state\"]\n            \ndrop_list = [col for col in df.columns if col not in keep_list]\ndf.drop(labels=drop_list, axis=1, inplace=True)\ndf.shape\n\n\n\ndf = df.rename(columns={\"loan_amnt\": \"loan_amount\", \"funded_amnt\": \"funded_amount\", \"funded_amnt_inv\": \"investor_funds\",\n                       \"int_rate\": \"interest_rate\", \"annual_inc\": \"annual_income\"})\n\n# Drop irrelevant columns\ndf.drop(['emp_title', 'zip_code', 'title'], axis=1, inplace=True)\n\ndf.columns.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['addr_state'].unique()\n\n# Make a list with each of the regions by state.\n\nwest = ['CA', 'OR', 'UT','WA', 'CO', 'NV', 'AK', 'MT', 'HI', 'WY', 'ID']\nsouth_west = ['AZ', 'TX', 'NM', 'OK']\nsouth_east = ['GA', 'NC', 'VA', 'FL', 'KY', 'SC', 'LA', 'AL', 'WV', 'DC', 'AR', 'DE', 'MS', 'TN' ]\nmid_west = ['IL', 'MO', 'MN', 'OH', 'WI', 'KS', 'MI', 'SD', 'IA', 'NE', 'IN', 'ND']\nnorth_east = ['CT', 'NY', 'PA', 'NJ', 'RI','MA', 'MD', 'VT', 'NH', 'ME']\n\n\n\ndf['region'] = np.nan\n\ndef finding_regions(state):\n    if state in west:\n        return 'West'\n    elif state in south_west:\n        return 'SouthWest'\n    elif state in south_east:\n        return 'SouthEast'\n    elif state in mid_west:\n        return 'MidWest'\n    elif state in north_east:\n        return 'NorthEast'\n    \n\n\ndf['region'] = df['addr_state'].apply(finding_regions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This code will take the current date and transform it into a year-month format\ndf['complete_date'] = pd.to_datetime(df['issue_d'])\n\ngroup_dates = df.groupby(['complete_date', 'region'], as_index=False).sum()\n\ngroup_dates['issue_d'] = [month.to_period('M') for \n                          month in group_dates['complete_date']]\n\ngroup_dates = group_dates.groupby(['issue_d', 'region'], as_index=False).sum()\ngroup_dates = group_dates.groupby(['issue_d', 'region'], as_index=False).sum()\ngroup_dates['loan_amount'] = group_dates['loan_amount']/1000\n\n\ndf_dates = pd.DataFrame(data=group_dates[['issue_d','region','loan_amount']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determining the loans that are bad from loan_status column\n\nbad_loan = [\"Charged Off\", \"Default\", \"In Grace Period\", \n            \"Late (16-30 days)\", \"Late (31-120 days)\"]\n\n\ndf['loan_condition'] = np.nan\n\ndef loan_condition(status):\n    if status in bad_loan:\n        return 'Bad Loan'\n    else:\n        return 'Good Loan'\n    \n    \ndf['loan_condition'] = df['loan_status'].apply(loan_condition)\n\n\n# We have 67429 loans categorized as bad loans\nbadloans_df = df.loc[df[\"loan_condition\"] == \"Bad Loan\"]\n\n# loan_status cross\nloan_status_cross = pd.crosstab(badloans_df['region'], badloans_df['loan_status']).apply(lambda x: x/x.sum() * 100)\nnumber_of_loanstatus = pd.crosstab(badloans_df['region'], badloans_df['loan_status'])\n\n\n# Round our values\nloan_status_cross['Charged Off'] = loan_status_cross['Charged Off'].apply(lambda x: round(x, 2))\nloan_status_cross['Default'] = loan_status_cross['Default'].apply(lambda x: round(x, 2))\n# loan_status_cross['Does not meet the credit policy. Status:Charged Off'] = loan_status_cross['Does not meet the credit policy. Status:Charged Off'].apply(lambda x: round(x, 2))\nloan_status_cross['In Grace Period'] = loan_status_cross['In Grace Period'].apply(lambda x: round(x, 2))\nloan_status_cross['Late (16-30 days)'] = loan_status_cross['Late (16-30 days)'].apply(lambda x: round(x, 2))\nloan_status_cross['Late (31-120 days)'] = loan_status_cross['Late (31-120 days)'].apply(lambda x: round(x, 2))\n\n\nnumber_of_loanstatus['Total'] = number_of_loanstatus.sum(axis=1) \n# number_of_badloans\nnumber_of_loanstatus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This code will take the current date and transform it into a year-month format\ndf['complete_date'] = pd.to_datetime(df['issue_d'])\n\ngroup_dates = df.groupby(['complete_date', 'region'], as_index=False).sum()\n\ngroup_dates['issue_d'] = [month.to_period('M') for \n                          month in group_dates['complete_date']]\n\ngroup_dates = group_dates.groupby(['issue_d', 'region'], as_index=False).sum()\ngroup_dates = group_dates.groupby(['issue_d', 'region'], as_index=False).sum()\ngroup_dates['loan_amount'] = group_dates['loan_amount']/1000\ndf['issue_d'].head()\ndt_series = pd.to_datetime(df['issue_d'])\ndf['year'] = dt_series.dt.year\n\ndf_dates = pd.DataFrame(data=group_dates[['issue_d','region','loan_amount']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employment_length = ['10+ years', '< 1 year', '1 year', '3 years', '8 years', '9 years',\n                    '4 years', '5 years', '6 years', '2 years', '7 years', 'n/a']\n\n# Create a new column and convert emp_length to integers.\n\nlst = [df]\ndf['emp_length_int'] = np.nan\n\nfor col in lst:\n    col.loc[col['emp_length'] == '10+ years', \"emp_length_int\"] = 10\n    col.loc[col['emp_length'] == '9 years', \"emp_length_int\"] = 9\n    col.loc[col['emp_length'] == '8 years', \"emp_length_int\"] = 8\n    col.loc[col['emp_length'] == '7 years', \"emp_length_int\"] = 7\n    col.loc[col['emp_length'] == '6 years', \"emp_length_int\"] = 6\n    col.loc[col['emp_length'] == '5 years', \"emp_length_int\"] = 5\n    col.loc[col['emp_length'] == '4 years', \"emp_length_int\"] = 4\n    col.loc[col['emp_length'] == '3 years', \"emp_length_int\"] = 3\n    col.loc[col['emp_length'] == '2 years', \"emp_length_int\"] = 2\n    col.loc[col['emp_length'] == '1 year', \"emp_length_int\"] = 1\n    col.loc[col['emp_length'] == '< 1 year', \"emp_length_int\"] = 0.5\n    col.loc[col['emp_length'] == 'n/a', \"emp_length_int\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loan issued by Region and by Credit Score grade\n# Change the colormap for tomorrow!\n\nsns.set_style('whitegrid')\n\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\ncmap = plt.cm.inferno\n\nby_interest_rate = df.groupby(['year', 'region']).interest_rate.mean()\nby_interest_rate.unstack().plot(kind='area', stacked=True, colormap=cmap, grid=False, legend=False, ax=ax1, figsize=(16,12))\nax1.set_title('Average Interest Rate by Region', fontsize=14)\n\n\nby_employment_length = df.groupby(['year', 'region']).emp_length_int.mean()\nby_employment_length.unstack().plot(kind='area', stacked=True, colormap=cmap, grid=False, legend=False, ax=ax2, figsize=(16,12))\nax2.set_title('Average Employment Length by Region', fontsize=14)\n# plt.xlabel('Year of Issuance', fontsize=14)\n\nby_dti = df.groupby(['year', 'region']).dti.mean()\nby_dti.unstack().plot(kind='area', stacked=True, colormap=cmap, grid=False, legend=False, ax=ax3, figsize=(16,12))\nax3.set_title('Average Debt-to-Income by Region', fontsize=14)\n\nby_income = df.groupby(['year', 'region']).annual_income.mean()\nby_income.unstack().plot(kind='area', stacked=True, colormap=cmap, grid=False, ax=ax4, figsize=(16,12))\nax4.set_title('Average Annual Income by Region', fontsize=14)\nax4.legend(bbox_to_anchor=(-1.0, -0.5, 1.8, 0.1), loc=10,prop={'size':12},\n           ncol=5, mode=\"expand\", borderaxespad=0.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have 67429 loans categorized as bad loans\nbadloans_df = df.loc[df[\"loan_condition\"] == \"Bad Loan\"]\n\n# loan_status cross\nloan_status_cross = pd.crosstab(badloans_df['region'], badloans_df['loan_status']).apply(lambda x: x/x.sum() * 100)\nnumber_of_loanstatus = pd.crosstab(badloans_df['region'], badloans_df['loan_status'])\n\n\n# Round our values\nloan_status_cross['Charged Off'] = loan_status_cross['Charged Off'].apply(lambda x: round(x, 2))\nloan_status_cross['Default'] = loan_status_cross['Default'].apply(lambda x: round(x, 2))\nloan_status_cross['In Grace Period'] = loan_status_cross['In Grace Period'].apply(lambda x: round(x, 2))\nloan_status_cross['Late (16-30 days)'] = loan_status_cross['Late (16-30 days)'].apply(lambda x: round(x, 2))\nloan_status_cross['Late (31-120 days)'] = loan_status_cross['Late (31-120 days)'].apply(lambda x: round(x, 2))\n\n\nnumber_of_loanstatus['Total'] = number_of_loanstatus.sum(axis=1) \n# number_of_badloans\nnumber_of_loanstatus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['interest_rate'].mean()\n# Average annual income of clients\ndf['annual_income'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create categories for annual_income since most of the bad loans are located below 100k\n\ndf['income_category'] = np.nan\nlst = [df]\n\nfor col in lst:\n    col.loc[col['annual_income'] <= 100000, 'income_category'] = 'Low'\n    col.loc[(col['annual_income'] > 100000) & (col['annual_income'] <= 200000), 'income_category'] = 'Medium'\n    col.loc[col['annual_income'] > 200000, 'income_category'] = 'High'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's transform the column loan_condition into integrers.\n\nlst = [df]\ndf['loan_condition_int'] = np.nan\n\nfor col in lst:\n    col.loc[df['loan_condition'] == 'Bad Loan', 'loan_condition_int'] = 0 # Negative (Bad Loan)\n    col.loc[df['loan_condition'] == 'Good Loan', 'loan_condition_int'] = 1 # Positive (Good Loan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4))= plt.subplots(nrows=2, ncols=2, figsize=(14,6))\n\n# Change the Palette types tomorrow!\n\nsns.violinplot(x=\"income_category\", y=\"loan_amount\", data=df, palette=\"Set2\", ax=ax1 )\nsns.violinplot(x=\"income_category\", y=\"loan_condition_int\", data=df, palette=\"Set2\", ax=ax2)\nsns.boxplot(x=\"income_category\", y=\"emp_length_int\", data=df, palette=\"Set2\", ax=ax3)\nsns.boxplot(x=\"income_category\", y=\"interest_rate\", data=df, palette=\"Set2\", ax=ax4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"by_condition = df.groupby('addr_state')['loan_condition'].value_counts()/ df.groupby('addr_state')['loan_condition'].count()\nby_emp_length = df.groupby(['region', 'addr_state'], as_index=False).emp_length_int.mean().sort_values(by=\"addr_state\")\nby_loan_amount = df.groupby(['region','addr_state'], as_index=False).loan_amount.sum()\nby_interest_rate = df.groupby(['region', 'addr_state'], as_index=False).interest_rate.mean()\nby_income = df.groupby(['region', 'addr_state'], as_index=False).annual_income.mean()\n\nloan_condition_bystate = pd.crosstab(df['addr_state'], df['loan_condition'] )\n\ncross_condition = pd.crosstab(df[\"addr_state\"], df[\"loan_condition\"])\n# Percentage of condition of loan\npercentage_loan_contributor = pd.crosstab(df['addr_state'], df['loan_condition']).apply(lambda x: x/x.sum() * 100)\ncondition_ratio = cross_condition[\"Bad Loan\"]/cross_condition[\"Good Loan\"]\nby_dti = df.groupby(['region', 'addr_state'], as_index=False).dti.mean()\nstates = by_loan_amount['addr_state'].values.tolist()\nfrom collections import OrderedDict\nstate_codes = sorted(states)\n\n\n# Take to a list\ndefault_ratio = condition_ratio.values.tolist()\naverage_dti = by_dti['dti'].values.tolist()\naverage_emp_length = by_emp_length[\"emp_length_int\"].values.tolist()\nnumber_of_badloans = loan_condition_bystate['Bad Loan'].values.tolist()\npercentage_ofall_badloans = percentage_loan_contributor['Bad Loan'].values.tolist()\n\n\n# Figure Number 2\nrisk_data = OrderedDict([('state_codes', state_codes),\n                         ('default_ratio', default_ratio),\n                         ('badloans_amount', number_of_badloans),\n                         ('percentage_of_badloans', percentage_ofall_badloans),\n                         ('average_dti', average_dti),\n                         ('average_emp_length', average_emp_length)])\n\n\n# Figure 2 Dataframe \nrisk_df = pd.DataFrame.from_dict(risk_data)\nrisk_df = risk_df.round(decimals=3)\nrisk_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['income_category'] = np.nan\nlst = [df]\n\nfor col in lst:\n    col.loc[col['annual_income'] <= 100000, 'income_category'] = 'Low'\n    col.loc[(col['annual_income'] > 100000) & (col['annual_income'] <= 200000), 'income_category'] = 'Medium'\n    col.loc[col['annual_income'] > 200000, 'income_category'] = 'High'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lst = [df]\ndf['loan_condition_int'] = np.nan\n\nfor col in lst:\n    col.loc[df['loan_condition'] == 'Bad Loan', 'loan_condition_int'] = 0 # Negative (Bad Loan)\n    col.loc[df['loan_condition'] == 'Good Loan', 'loan_condition_int'] = 1 # Positive (Good Loan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4))= plt.subplots(nrows=2, ncols=2, figsize=(14,6))\n\n# Change the Palette types tomorrow!\n\nsns.violinplot(x=\"income_category\", y=\"loan_amount\", data=df, palette=\"Set2\", ax=ax1 )\nsns.violinplot(x=\"income_category\", y=\"loan_condition_int\", data=df, palette=\"Set2\", ax=ax2)\nsns.boxplot(x=\"income_category\", y=\"emp_length_int\", data=df, palette=\"Set2\", ax=ax3)\nsns.boxplot(x=\"income_category\", y=\"interest_rate\", data=df, palette=\"Set2\", ax=ax4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['interest_rate'].describe()\n# Average interest is 13.26% Anything above this will be considered of high risk let's see if this is true.\ndf['interest_payments'] = np.nan\nlst = [df]\n\nfor col in lst:\n    col.loc[col['interest_rate'] <= 13.23, 'interest_payments'] = 'Low'\n    col.loc[col['interest_rate'] > 13.23, 'interest_payments'] = 'High'\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"complete_df = df.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Handling Missing Numeric Values\n\n# Transform Missing Values for numeric dataframe\n# Nevertheless check what these variables mean tomorrow in the morning.\n# for col in ('dti_joint', 'annual_inc_joint', 'il_util', 'mths_since_rcnt_il', 'open_acc_6m', 'open_il_12m',\n#            'open_il_24m', 'inq_last_12m', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl',\n#            'mths_since_last_record', 'mths_since_last_major_derog', 'mths_since_last_delinq', 'total_bal_il', 'tot_coll_amt',\n#            'tot_cur_bal', 'total_rev_hi_lim', 'revol_util', 'collections_12_mths_ex_med', 'open_acc', 'inq_last_6mths',\n#            'verification_status_joint', 'acc_now_delinq'):\n#     complete_df[col] = complete_df[col].fillna(0)\n    \n\n\n# # Get the mode of next payment date and last payment date and the last date credit amount was pulled   \n# complete_df[\"next_pymnt_d\"] = complete_df.groupby(\"region\")[\"next_pymnt_d\"].transform(lambda x: x.fillna(x.mode))\n# complete_df[\"last_pymnt_d\"] = complete_df.groupby(\"region\")[\"last_pymnt_d\"].transform(lambda x: x.fillna(x.mode))\n# complete_df[\"last_credit_pull_d\"] = complete_df.groupby(\"region\")[\"last_credit_pull_d\"].transform(lambda x: x.fillna(x.mode))\n# complete_df[\"earliest_cr_line\"] = complete_df.groupby(\"region\")[\"earliest_cr_line\"].transform(lambda x: x.fillna(x.mode))\n\n# # Get the mode on the number of accounts in which the client is delinquent\ncomplete_df[\"pub_rec\"] = complete_df.groupby(\"region\")[\"pub_rec\"].transform(lambda x: x.fillna(x.median()))\n\n# # Get the mean of the annual income depending in the region the client is located.\ncomplete_df[\"annual_income\"] = complete_df.groupby(\"region\")[\"annual_income\"].transform(lambda x: x.fillna(x.mean()))\n\n# Get the mode of the  total number of credit lines the borrower has \ncomplete_df[\"total_acc\"] = complete_df.groupby(\"region\")[\"total_acc\"].transform(lambda x: x.fillna(x.median()))\n\n# Mode of credit delinquencies in the past two years.\n# complete_df[\"delinq_2yrs\"] = complete_df.groupby(\"region\")[\"delinq_2yrs\"].transform(lambda x: x.fillna(x.mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndt_series = pd.to_datetime(complete_df['earliest_cr_line'])\ncomplete_df['earliest_cr_line'] = dt_series.dt.year\n\ncomplete_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# complete_df['loan_condition'].value_counts\n\n# complete_df.groupby('loan_condition').count()\n\ncomplete_df['Bad_Loan'] = np.where(complete_df['loan_condition']== 'Bad Loan', 1, 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncomplete_df.to_csv(\"complete_df.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict, train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.read_csv(\n    '../input/newddasdsa/complete_df.csv')\n\ndf = df.drop('loan_condition_int', 1)\ndf = df.drop('loan_condition', 1)\ndf = df.drop('id', 1)\ndf = df.drop('complete_date', 1)\ndf['term'] = df['term'].map(lambda x: x.rstrip('months'))\ndf[\"term\"] = pd.to_numeric(df[\"term\"])\ndf = df.dropna(how='any',axis=0) \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop('inq_last_12m', 1)\ndf = df.drop('inq_fi', 1)\ndf = df.drop('total_rev_hi_lim', 1)\ndf = df.drop('out_prncp_inv', 1)\ndf = df.drop('out_prncp', 1)\ndf = df.drop('fico_range_low', 1)\ndf = df.drop('total_bal_il', 1)\ndf = df.drop('il_util', 1)\ndf = df.drop('open_il_12m', 1)\ndf = df.drop('open_il_24m', 1)\ndf = df.drop('open_acc_6m', 1)\ndf = df.drop('pub_rec', 1)\ndf = df.drop('grade', 1)\ndf = df.drop('sub_grade', 1)\ndf = df.drop('earliest_cr_line', 1)\ndf = df.drop('tot_coll_amt', 1)\ndf = df.drop('pub_rec_bankruptcies', 1)\ndf = df.drop('mort_acc', 1)\ndf = df.drop('open_rv_12m', 1)\ndf = df.drop('open_rv_24m', 1)\ndf = df.drop('revol_bal', 1)\ndf = df.drop('revol_util', 1)\ndf = df.drop('total_acc', 1)\ndf = df.drop('emp_length', 1)\ndf = df.drop('open_acc', 1)\ndf = df.drop('annual_inc_joint', 1)\ndf = df.drop('home_ownership', 1)\ndf = df.drop('dti_joint', 1)\ndf = df.drop('verification_status', 1)\ndf = df.drop('initial_list_status', 1)\ndf = df.drop('application_type', 1)\ndf = df.drop('purpose', 1)\ndf = df.drop('Unnamed: 0', 1)\ndf = df.drop('issue_d', 1)\ndf = df.drop('emp_length_int', 1)\ndf = df.drop('income_category', 1)\ndf = df.drop('interest_payments', 1)\ndf = df.drop('region', 1)\ndf = df.drop('year', 1)\ndf = df.drop('fico_range_high', 1)\ndf = df.drop('addr_state', 1)\ndf = df.drop('interest_rate', 1)\ndf = df.drop('annual_income', 1)\nlist(df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Determine Numeric & Categorical Columns\nnumeric=df.columns[df.dtypes==\"float64\"]\ncat=df.columns[df.dtypes==\"object\"]\n\n#Normalize numeric columns and create dummy variables with categorical columns\n\nscaler=StandardScaler()\ndf[numeric] = scaler.fit_transform(df[numeric])\ndf=pd.get_dummies(df,drop_first=True)\nX = df.drop('Bad_Loan', axis=1)\ny = df['Bad_Loan']\nX_train, X_test, y_train, y_test = train_test_split(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TensorFlow Model \n\n\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.python.framework import ops\ntf.compat.v1.disable_eager_execution()\nimport numpy as np\n\n# Reset the graph for Tensorboard\ndef reset_graph(seed=42):\n    tf.reset_default_graph()\n    tf.set_random_seed(seed)\n    np.random.seed(seed)\n    \n\n\n# Variables\nn_inputs = X_train.shape[1]\nn_hidden1 = 15\nn_hidden2 = 5\nn_outputs = 2\n\n# Reset the tensorboard graph\nreset_graph()\n\n\n# Placeholders\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\ny = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n\n\n# Structure of the Neural Network\nwith tf.name_scope(\"dnn\"):\n    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n                             activation=tf.nn.relu)\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n                             activation=tf.nn.relu)\n    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n\n# Cost Function\nwith tf.name_scope(\"loss\"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n                                                     logits=logits) # Difference from logits and actual y values\n    loss = tf.reduce_mean(xentropy) # Get the average of the loss for each instance.\n\n# Gradient Descent\nlearning_rate = 0.01\n\nwith tf.name_scope(\"train\"):\n    optimization = tf.train.GradientDescentOptimizer(learning_rate) # Determine the level of steps in gradient descent process.\n    training_op = optimization.minimize(loss) # Get the training set with parameters that obtain the minimum loss.\n\n# Evaluation\nwith tf.name_scope(\"eval\"):\n    correct = tf.nn.in_top_k(logits, y, 1) # Did the highest score of logit is equivalent to the actual value(returns booleans)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # We want the mean of the loss for every instance.\n    \n\n\n# Init and Saver\ninit = tf.global_variables_initializer() # This variable initializes all variables.\nsaver = tf.train.Saver() # Saves the training set \n\n\namnt_epochs = 5\nbatch_size = 100\n\nwith tf.Session() as sess:\n    init.run()\n    \n    for epoch in range(amnt_epochs):\n        epoch_loss = 0\n        i=0\n        while i < len(X_train):\n            start = i\n            end = i+batch_size\n            batch_x = np.array(X_train[start:end])\n            batch_y = np.array(y_train[start:end])\n\n            _, c = sess.run([training_op, loss], feed_dict={X: batch_x,\n                                              y: batch_y})\n            epoch_loss += c\n            i+=batch_size\n        acc_train = accuracy.eval(feed_dict={X: batch_x, y: batch_y})\n        acc_test = accuracy.eval(feed_dict={X: X_test, y:y_test})\n\n        print(epoch+1, 'Train accuracy: ', acc_train, 'Test accuracy: ', acc_test, 'Loss: ', epoch_loss)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}